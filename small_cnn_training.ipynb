{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c5bdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in c:\\users\\sarat\\appdata\\roaming\\python\\python313\\site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: torch==2.8.0 in c:\\users\\sarat\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.8.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.8.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.8.0->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.8.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.8.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.8.0->torchvision) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.8.0->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision\n",
    "\n",
    "import os, math, random, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7fa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def _compute_mean_std(torchvision_dataset):\n",
    "    loader = DataLoader(torchvision_dataset, batch_size=512, shuffle=False, num_workers=2)\n",
    "    mean = 0.0; std = 0.0; n = 0\n",
    "    for x, _ in loader:\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, x.size(1), -1)         # [B, C, H*W]\n",
    "        mean += x.mean(2).sum(0)              # sum of per-sample means\n",
    "        std  += x.std(2, unbiased=False).sum(0)\n",
    "        n += b\n",
    "    mean /= n; std /= n\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "def get_dataloaders(dataset=\"MNIST\", batch_size=128, max_train=None, max_test=None,\n",
    "                    norm_mode=\"standardize\"):\n",
    "    \"\"\"\n",
    "    norm_mode:\n",
    "      - \"standardize\": compute train-set mean/std (StandardScaler-style)\n",
    "      - \"minus1to1\": map [0,1] -> [-1,1] using mean=(0.5,)*C, std=(0.5,)*C\n",
    "      - \"none\": only ToTensor()\n",
    "    \"\"\"\n",
    "    name = dataset.upper()\n",
    "    if name in [\"MNIST\", \"FASHIONMNIST\", \"FASHION-MNIST\"]:\n",
    "        DS = datasets.FashionMNIST if \"FASHION\" in name else datasets.MNIST\n",
    "        base_train = DS(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "        if norm_mode == \"standardize\":\n",
    "            mean, std = _compute_mean_std(base_train)\n",
    "            norm = transforms.Normalize(mean, std)\n",
    "        elif norm_mode == \"minus1to1\":\n",
    "            norm = transforms.Normalize((0.5,), (0.5,))\n",
    "        elif norm_mode == \"none\":\n",
    "            norm = None\n",
    "        else:\n",
    "            raise ValueError(\"Unknown norm_mode\")\n",
    "\n",
    "        tform = transforms.Compose([transforms.ToTensor()] + ([norm] if norm else []))\n",
    "        train_set = DS(root=\"./data\", train=True,  download=True, transform=tform)\n",
    "        test_set  = DS(root=\"./data\", train=False, download=True, transform=tform)\n",
    "\n",
    "    elif name == \"CIFAR10\":\n",
    "        base_train = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "        if norm_mode == \"standardize\":\n",
    "            mean, std = _compute_mean_std(base_train)\n",
    "            norm = transforms.Normalize(mean, std)\n",
    "        elif norm_mode == \"minus1to1\":\n",
    "            norm = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        elif norm_mode == \"none\":\n",
    "            norm = None\n",
    "        else:\n",
    "            raise ValueError(\"Unknown norm_mode\")\n",
    "\n",
    "        tform = transforms.Compose([transforms.ToTensor()] + ([norm] if norm else []))\n",
    "        train_set = datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=tform)\n",
    "        test_set  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tform)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset.\")\n",
    "\n",
    "    if max_train: train_set = Subset(train_set, list(range(min(max_train, len(train_set)))))\n",
    "    if max_test:  test_set  = Subset(test_set,  list(range(min(max_test,  len(test_set)))))\n",
    "\n",
    "    return (\n",
    "        DataLoader(train_set, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True),\n",
    "        DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1490124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10, activation=\"relu\", p_drop=0.0, input_size=28):\n",
    "        super().__init__()\n",
    "\n",
    "        def make_act(name):\n",
    "            return {\n",
    "                \"relu\": nn.ReLU,\n",
    "                \"tanh\": nn.Tanh,\n",
    "                \"gelu\": nn.GELU,\n",
    "                \"lrelu\": lambda: nn.LeakyReLU(0.1),\n",
    "            }[name]()  # fresh instance each time\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1), make_act(activation), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),          make_act(activation), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),         make_act(activation), nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "        # compute flattened size dynamically (works for 28x28, 32x32, etc.)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, input_size, input_size)\n",
    "            flat = self.features(dummy).view(1, -1).size(1)\n",
    "\n",
    "        #### SINGLE LAYER OF FULLY CONNECTED LAYER\n",
    "      #  self.fc = nn.Sequential(\n",
    "      #      nn.Linear(flat, 128), make_act(activation),\n",
    "      #      nn.Linear(128, num_classes),\n",
    "      #  )\n",
    " #### SINGLE LAYER OF FULLY CONNECTED LAYER\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flat, 128), make_act(activation),\n",
    "            nn.Linear(128, 64), make_act(activation),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.features(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.dropout(z)\n",
    "        return self.fc(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abc15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, device, criterion, scaler=None, scheduler=None, grad_clip=None):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "    autocast_ctx = torch.cuda.amp.autocast if (scaler is not None and device.type == \"cuda\") else nullcontext\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with autocast_ctx():\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        if scaler is None:\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "        else:\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # metrics\n",
    "        loss_sum += float(loss) * y.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "from contextlib import nullcontext\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += float(loss) * y.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return loss_sum / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd9c87f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train loss 1.902 acc 0.296 | test loss 1.653 acc 0.399\n",
      "Epoch 01 | train loss 1.419 acc 0.479 | test loss 1.375 acc 0.503\n",
      "Epoch 02 | train loss 1.188 acc 0.578 | test loss 1.153 acc 0.596\n",
      "Epoch 03 | train loss 1.014 acc 0.641 | test loss 1.083 acc 0.629\n",
      "Epoch 04 | train loss 0.891 acc 0.686 | test loss 1.053 acc 0.643\n",
      "Epoch 05 | train loss 0.746 acc 0.736 | test loss 1.011 acc 0.663\n",
      "Epoch 06 | train loss 0.678 acc 0.763 | test loss 1.053 acc 0.675\n",
      "Epoch 07 | train loss 0.590 acc 0.794 | test loss 1.042 acc 0.670\n",
      "Epoch 08 | train loss 0.509 acc 0.822 | test loss 1.245 acc 0.624\n",
      "Epoch 09 | train loss 0.456 acc 0.844 | test loss 1.308 acc 0.650\n",
      "Epoch 10 | train loss 0.417 acc 0.854 | test loss 1.268 acc 0.660\n",
      "Epoch 11 | train loss 0.378 acc 0.872 | test loss 1.254 acc 0.670\n",
      "Epoch 12 | train loss 0.371 acc 0.874 | test loss 1.382 acc 0.644\n",
      "Epoch 13 | train loss 0.352 acc 0.882 | test loss 1.486 acc 0.669\n",
      "Epoch 14 | train loss 0.337 acc 0.889 | test loss 1.479 acc 0.658\n",
      "Epoch 15 | train loss 0.316 acc 0.898 | test loss 1.487 acc 0.663\n",
      "Epoch 16 | train loss 0.288 acc 0.908 | test loss 1.731 acc 0.650\n",
      "Epoch 17 | train loss 0.320 acc 0.897 | test loss 1.602 acc 0.656\n",
      "Epoch 18 | train loss 0.291 acc 0.911 | test loss 1.701 acc 0.659\n",
      "Epoch 19 | train loss 0.309 acc 0.902 | test loss 1.829 acc 0.646\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed(7)\n",
    "    device = get_device()\n",
    "\n",
    "    ## for MNIST\n",
    "    #train_loader, test_loader = get_dataloaders(dataset=\"MNIST\", batch_size=128, max_train=10000, max_test=2000)\n",
    "\n",
    "    #model = SmallCNN(in_channels=1, num_classes=10, activation=\"relu\", p_drop=0.0, input_size=28).to(device)\n",
    "\n",
    "    ## for FASHIONMNIST\n",
    "    #train_loader, test_loader = get_dataloaders(dataset=\"FASHIONMNIST\", batch_size=128, max_train=10000, max_test=2000)\n",
    "\n",
    "    #model = SmallCNN(in_channels=1, num_classes=10, activation=\"relu\", p_drop=0.0, input_size=28).to(device)\n",
    "    ## for CIFAR10\n",
    "    train_loader, test_loader = get_dataloaders(dataset=\"CIFAR10\", batch_size=128, max_train=20000, max_test=5000)\n",
    "    model = SmallCNN(in_channels=3, num_classes=10, activation=\"relu\", p_drop=0.0, input_size=32).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=0.0)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device, criterion)\n",
    "        te_loss, te_acc = evaluate(model, test_loader, device, criterion)\n",
    "        print(f\"Epoch {epoch:02d} | train loss {tr_loss:.3f} acc {tr_acc:.3f} | test loss {te_loss:.3f} acc {te_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b6abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
