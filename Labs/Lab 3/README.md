ğŸ«€ Fine-Tuning Pre-Trained Models for ECG Analysis with PEFT
ğŸ“Š Dataset Overview

Dataset Name: df_segment2.csv

Shape: 65,425 Ã— 601

Description:

Each row represents one ECG segment.

Each segment contains 600 signal samples followed by a binary label (0 or 1) in the last column.

Segments originate from multiple users (user IDs are not provided to students).

ğŸ“¥ Download

You can download the dataset from:
ğŸ‘‰ USF Learn â€“ df_segment2.csv

Note: You must be logged into your USF Learn account to access the file.

ğŸ§ª Preprocessing (Required)

Before feeding the data into models, you must normalize each segment:

âœ… Per-segment z-score (Recommended)
For each row 
ğ‘¥
x:

ğ‘¥
â€²
=
ğ‘¥
âˆ’
ğœ‡
row
ğœ
row
x
â€²
=
Ïƒ
row
	â€‹

xâˆ’Î¼
row
	â€‹

	â€‹


ğŸ§ª Global z-score (For Ablation)
Compute the global mean and standard deviation over the entire dataset and normalize all samples accordingly.

ğŸ§  Assignment Overview

The goal of this assignment is to fine-tune pre-trained audio and foundation models for ECG signal analysis using Parameter-Efficient Fine-Tuning (PEFT) methods such as Adapters and LoRA.

ğŸ¯ Objective

Fine-tune Wav2Vec2, HuBERT, and the ECG-FM foundation model for binary classification of ECG signals.

ğŸ“š Models to use:

Wav2Vec2

HuBERT

ECG-FM

ğŸ“ Tasks
1. Model Selection & Preparation

Choose two pre-trained models (Wav2Vec2 and HuBERT) and include ECG-FM.

Study their architectures, expected input format, and capabilities.

Preprocess the ECG data to match each model's input requirements (e.g., reshape 1D signals as needed).

2. Fine-Tuning with PEFT

Implement and compare two PEFT techniques for each model:

ğŸ§© Adapters

ğŸª„ LoRA (Low-Rank Adaptation)

3. Implementation & Training

Use PyTorch (preferred) or TensorFlow.

Implement fine-tuning for each model with adapters and LoRA.

Tune hyperparameters such as learning rate, batch size, epochs, and regularization.

Log training and validation curves for analysis.

4. Evaluation & Comparison

Evaluate models using:

Accuracy

Precision

Recall

F1-score

Compare adapters vs LoRA for each model and analyze trade-offs in performance, convergence, and resource usage.

ğŸ“¦ Deliverables

ğŸ“„ Report (PDF or Markdown)

Overview of models and their architectures.

Description of PEFT techniques used.

ECG preprocessing pipeline.

Training process and hyperparameters.

Quantitative results and performance comparison.

ğŸ’» Code

Implementation scripts or notebooks for each model Ã— PEFT combination.

Clear documentation and comments.

ğŸ§® Grading Criteria
Category	Points	Description
Model Understanding & Selection	40 pts	Correct model choice, architectural understanding, PEFT justification
Implementation & Training	60 pts	Proper implementation and training with adapters/LoRA
Evaluation & Comparison	60 pts	Metric computation, adapters vs LoRA comparison
Report Quality & Clarity	40 pts	Clear, organized, well-written report with insights
ğŸ“š References

Hugging Face Transformers

LoRA: Low-Rank Adaptation of Large Language Models

ECG-FM Foundation Model
