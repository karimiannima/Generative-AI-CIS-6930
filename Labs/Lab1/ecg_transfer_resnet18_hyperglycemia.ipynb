{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1cfe29-1e67-427c-99ee-a4aa3bdc4bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: neurokit2 in c:\\users\\sarat\\appdata\\roaming\\python\\python313\\site-packages (0.2.12)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from neurokit2) (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from neurokit2) (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from neurokit2) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from neurokit2) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from neurokit2) (1.6.1)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from neurokit2) (3.10.0)\n",
      "Requirement already satisfied: PyWavelets>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from neurokit2) (1.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->neurokit2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->neurokit2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->neurokit2) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->neurokit2) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->neurokit2) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->neurokit2) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->neurokit2) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->neurokit2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->neurokit2) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->neurokit2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->neurokit2) (3.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->neurokit2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->neurokit2) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->neurokit2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->neurokit2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->neurokit2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->neurokit2) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install neurokit2\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb4269c-c374-4724-a532-bca2b0f27a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Drop-in: record loader (patient-wise split) ---\n",
    "import os, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _index_signal_files(root):\n",
    "    \"\"\"Map filename stem -> full path, recursively (.csv/.CSV/.txt/.TXT).\"\"\"\n",
    "    root = Path(root)\n",
    "    base_to_path = {}\n",
    "    for pat in (\"*.csv\",\"*.CSV\",\"*.txt\",\"*.TXT\"):\n",
    "        for p in root.rglob(pat):\n",
    "            base_to_path[p.stem] = str(p)\n",
    "    return base_to_path\n",
    "\n",
    "def _read_signal_csv_any(path):\n",
    "    \"\"\"Return (T,) or (L,T) numeric array; transpose to (L,T) when needed.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        num = df.select_dtypes(include=[np.number])\n",
    "        if num.empty:\n",
    "            raise ValueError\n",
    "        arr = num.to_numpy()\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        num = df.select_dtypes(include=[np.number])\n",
    "        arr = num.to_numpy()\n",
    "    if arr.ndim == 1:\n",
    "        return arr.astype(np.float32)\n",
    "    T, C = arr.shape\n",
    "    if T >= C:   # rows are time -> transpose to (L,T)\n",
    "        arr = arr.T\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def load_nguyen_dataset(label_csv, signal_dir, fs=1000, test_size=0.2, random_state=1337,\n",
    "                        id_col=\"Id\", label_col=\"Hyperglycemia\"):\n",
    "    \"\"\"\n",
    "    Returns: records_train, labels_train, records_val, labels_val\n",
    "    - Patient-wise split using the prefix before '_' in Id (e.g., '003' from '003_1')\n",
    "    \"\"\"\n",
    "    labels = pd.read_csv(label_csv)\n",
    "    labels[id_col] = labels[id_col].astype(str)\n",
    "    labels[\"patient\"] = labels[id_col].apply(lambda s: re.split(r\"[_\\-]\", s)[0])\n",
    "\n",
    "    idx = _index_signal_files(signal_dir)\n",
    "    labels[\"path\"] = labels[id_col].map(idx.get)\n",
    "    labels = labels.loc[labels[\"path\"].notna()].reset_index(drop=True)\n",
    "    if len(labels) == 0:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No signal files in '{signal_dir}' matched Id stems from '{label_csv}'. \"\n",
    "            \"Expected files like '003_1.csv'.\"\n",
    "        )\n",
    "\n",
    "    y = labels[label_col].astype(str).str.lower().isin([\"1\",\"true\",\"yes\"]).astype(int).to_numpy()\n",
    "    groups = labels[\"patient\"].to_numpy()\n",
    "    paths  = labels[\"path\"].tolist()\n",
    "\n",
    "    print(\"Reading ECG files ...\")\n",
    "    ecgs = [_read_signal_csv_any(p) for p in tqdm(paths)]\n",
    "\n",
    "    # patient-wise split; try to ensure both classes appear in both sets\n",
    "    N = len(ecgs); idxs = np.arange(N); ok = False\n",
    "    for k in range(10):\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state+k)\n",
    "        tr_idx, va_idx = next(gss.split(idxs, groups=groups))\n",
    "        if len(tr_idx) and len(va_idx) and len(np.unique(y[tr_idx]))>=2 and len(np.unique(y[va_idx]))>=2:\n",
    "            ok = True; break\n",
    "    if not ok:\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state+99)\n",
    "        tr_idx, va_idx = next(gss.split(idxs, groups=groups))\n",
    "\n",
    "    records_train = [ecgs[i] for i in tr_idx]\n",
    "    labels_train  = [int(y[i]) for i in tr_idx]\n",
    "    records_val   = [ecgs[i] for i in va_idx]\n",
    "    labels_val    = [int(y[i]) for i in va_idx]\n",
    "\n",
    "    print(f\"Loaded {len(records_train)} train / {len(records_val)} val \"\n",
    "          f\"(positives: {sum(labels_train)} / {sum(labels_val)})\")\n",
    "    return records_train, labels_train, records_val, labels_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d251a41-5f65-4c0f-9c14-cf950652c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_rpeaks_neurokit(x, fs=1000):\n",
    "    \"\"\"\n",
    "    x: 1-D numpy array (single lead) used for detection.\n",
    "       If you have multi-lead, pass a good lead for R detection (e.g., lead II).\n",
    "    returns: sorted R-peak sample indices (np.ndarray)\n",
    "    \"\"\"\n",
    "    x_clean = nk.ecg_clean(x, sampling_rate=fs, method=\"neurokit\")\n",
    "    # ecg_peaks expects raw/cleaned 1D signal\n",
    "    _, info = nk.ecg_peaks(x_clean, sampling_rate=fs)\n",
    "    rpeaks = np.asarray(info.get(\"ECG_R_Peaks\", []), dtype=int)\n",
    "    return np.sort(rpeaks)\n",
    "\n",
    "def segment_beats(x, rpeaks, fs=1000, pre_ms=200, post_ms=400, discard_first_last=True):\n",
    "    \"\"\"\n",
    "    x: (T,) or (L, T)\n",
    "    rpeaks: np.ndarray of R-peak sample indices\n",
    "    returns: list of beat segments (same shape as x but time-cropped to [R-pre, R+post])\n",
    "             and (start,end) indices kept\n",
    "    \"\"\"\n",
    "    if rpeaks is None or len(rpeaks) == 0:\n",
    "        return [], []\n",
    "    if discard_first_last and len(rpeaks) >= 2:\n",
    "        rpeaks = rpeaks[1:-1]\n",
    "\n",
    "    w_pre  = int(round(fs * pre_ms  / 1000.0))\n",
    "    w_post = int(round(fs * post_ms / 1000.0))\n",
    "    T = x.shape[-1]\n",
    "    beats, spans = [], []\n",
    "    for r in rpeaks:\n",
    "        s = r - w_pre\n",
    "        e = r + w_post\n",
    "        if s < 0 or e > T:\n",
    "            continue\n",
    "        seg = x[..., s:e]     # supports (T,) or (L,T)\n",
    "        beats.append(seg.copy())\n",
    "        spans.append((s, e))\n",
    "    return beats, spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34aee82e-7f16-4f16-abd1-f8ceb621233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class ECGBeatsImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Builds beat-centered spectrogram images per record.\n",
    "    - records: list of np.ndarrays, each (T,) or (L,T)\n",
    "    - labels:  list of ints (0/1) per record (e.g., hyperglycemia)\n",
    "    - detect_lead: which lead index to use for R detection if multi-lead\n",
    "    - max_beats_per_record: cap to avoid over-representing long records\n",
    "    \"\"\"\n",
    "    def __init__(self, records, labels, fs=1000, detect_lead=0,\n",
    "                 pre_ms=200, post_ms=400, max_beats_per_record=20,\n",
    "                 discard_first_last=True, leads_idx=(0,1,2)):\n",
    "        assert len(records) == len(labels)\n",
    "        self.fs = fs\n",
    "        self.labels = labels\n",
    "        self.items = []  # list of (record_idx, beat_segment_np)\n",
    "        self.rec_index = []  # parallel list mapping to record idx (for eval aggregation)\n",
    "        self.leads_idx = leads_idx\n",
    "\n",
    "        for ri, sig in enumerate(records):\n",
    "            sig = np.asarray(sig)\n",
    "            # Choose 1 lead for detection\n",
    "            if sig.ndim == 1:\n",
    "                detect_sig = sig\n",
    "            else:\n",
    "                L = sig.shape[0]\n",
    "                li = min(detect_lead, L-1)\n",
    "                detect_sig = sig[li]\n",
    "\n",
    "            rpeaks = detect_rpeaks_neurokit(detect_sig, fs=fs)\n",
    "            beats, _ = segment_beats(sig, rpeaks, fs=fs, pre_ms=pre_ms, post_ms=post_ms,\n",
    "                                     discard_first_last=discard_first_last)\n",
    "            if len(beats) == 0:\n",
    "                continue\n",
    "            # Cap beats per record for balance\n",
    "            if max_beats_per_record is not None and len(beats) > max_beats_per_record:\n",
    "                sel = np.linspace(0, len(beats)-1, max_beats_per_record, dtype=int)\n",
    "                beats = [beats[i] for i in sel]\n",
    "            for b in beats:\n",
    "                self.items.append((ri, b))\n",
    "                self.rec_index.append(ri)\n",
    "\n",
    "        self.y_per_record = np.asarray(labels, dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ri, beat = self.items[i]\n",
    "        # beat: (T,) or (L,T) -> image via your spectrogram\n",
    "        img = ecg_to_logspec(beat, fs=self.fs, leads_idx=self.leads_idx)  # (3,H,W) tensor\n",
    "        y = int(self.y_per_record[ri])  # record label -> beat label\n",
    "        return img, y, ri  # include record idx for eval aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b308b5b3-bc9a-4720-8160-a15ea32dd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model builder (ResNet-18, ImageNet-pretrained) ---\n",
    "import torch, torch.nn as nn\n",
    "import torchvision as tv\n",
    "\n",
    "# Safe to redefine; keeps your notebook self-contained\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def make_model(num_classes: int = 2, freeze_backbone: bool = True) -> nn.Module:\n",
    "    \"\"\"ResNet-18 with a new classification head.\"\"\"\n",
    "    try:\n",
    "        # Newer torchvision (>=0.13)\n",
    "        weights = tv.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        model = tv.models.resnet18(weights=weights)\n",
    "    except Exception:\n",
    "        # Fallback for older torchvision\n",
    "        model = tv.models.resnet18(pretrained=True)\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_feats, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d814886f-2cbf-4b89-8b56-5f25e7b542a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score, classification_report\n",
    "\n",
    "def train_one_epoch_beats(model, loader, opt, criterion, device=\"cuda\"):\n",
    "    model.train()\n",
    "    total, loss_sum, y_true, y_pred = 0, 0.0, [], []\n",
    "    for x, y, _ in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += float(loss) * y.size(0)\n",
    "        total   += y.size(0)\n",
    "        y_true += y.cpu().tolist()\n",
    "        y_pred += torch.argmax(logits, 1).cpu().tolist()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average='macro')\n",
    "    return loss_sum/total, acc, f1m\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_by_record(model, loader, criterion, device=\"cuda\", class_names=(\"normal\",\"hyper\")):\n",
    "    model.eval()\n",
    "    total, loss_sum = 0, 0.0\n",
    "    rec_probs = defaultdict(list)\n",
    "    rec_true  = {}\n",
    "\n",
    "    for x, y, rec_idx in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # positive class prob\n",
    "\n",
    "        loss_sum += float(loss) * y.size(0)\n",
    "        total += y.size(0)\n",
    "\n",
    "        rec_idx = rec_idx.cpu().numpy()\n",
    "        y_np = y.cpu().numpy()\n",
    "        for r, p, t in zip(rec_idx, probs, y_np):\n",
    "            rec_probs[int(r)].append(float(p))\n",
    "            rec_true[int(r)] = int(t)\n",
    "\n",
    "    # Aggregate per record (mean prob)\n",
    "    rec_ids = sorted(rec_true.keys())\n",
    "    y_true_rec = np.array([rec_true[r] for r in rec_ids])\n",
    "    y_score_rec = np.array([np.mean(rec_probs[r]) for r in rec_ids])\n",
    "    y_hat_rec = (y_score_rec >= 0.5).astype(int)\n",
    "\n",
    "    acc  = accuracy_score(y_true_rec, y_hat_rec)\n",
    "    f1m  = f1_score(y_true_rec, y_hat_rec, average='macro')\n",
    "    auroc = roc_auc_score(y_true_rec, y_score_rec) if len(set(y_true_rec))==2 else float('nan')\n",
    "    auprc = average_precision_score(y_true_rec, y_score_rec) if len(set(y_true_rec))==2 else float('nan')\n",
    "\n",
    "    print(\"\\nRecord-level classification report:\")\n",
    "    print(classification_report(y_true_rec, y_hat_rec, target_names=class_names, digits=3))\n",
    "    return (loss_sum/total), acc, f1m, auroc, auprc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c97464-adc1-47d4-96a2-6e9be4eb1e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ECG files ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1670/1670 [00:08<00:00, 185.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1335 train / 335 val (positives: 671 / 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Point these at your folder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "DATA_DIR   = r\"C:\\Users\\sarat\\Desktop\\ECG\\x_nguyen\"\n",
    "LABEL_CSV  = rf\"{DATA_DIR}\\ecg-ForNguyenSimulation-with-label.csv\"\n",
    "SIGNAL_DIR = DATA_DIR\n",
    "\n",
    "# 1) Load records (patient-wise)\n",
    "records_train, labels_train, records_val, labels_val = load_nguyen_dataset(\n",
    "    LABEL_CSV, SIGNAL_DIR, fs=1000, test_size=0.2\n",
    ")\n",
    "\n",
    "# 2) Build beat-image datasets (R-peaks via NeuroKit2; discard first/last)\n",
    "beat_train = ECGBeatsImageDataset(records_train, labels_train, fs=1000,\n",
    "                                  detect_lead=0, pre_ms=200, post_ms=400,\n",
    "                                  max_beats_per_record=20,\n",
    "                                  discard_first_last=True, leads_idx=(0,1,2))\n",
    "beat_val   = ECGBeatsImageDataset(records_val,   labels_val,   fs=1000,\n",
    "                                  detect_lead=0, pre_ms=200, post_ms=400,\n",
    "                                  max_beats_per_record=None,\n",
    "                                  discard_first_last=True, leads_idx=(0,1,2))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "tr_loader = DataLoader(beat_train, batch_size=64, sampler=None, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "va_loader = DataLoader(beat_val,   batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "# 3) Train as you had:\n",
    "model = make_model(num_classes=2, freeze_backbone=True).to(DEVICE)\n",
    "\n",
    "# class-weighted loss from record labels\n",
    "\n",
    "classes = np.array([0,1])\n",
    "cw = compute_class_weight(\"balanced\", classes=classes, y=np.array(labels_train))\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(cw, dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "opt = torch.optim.AdamW(model.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "for ep in range(1, 5+1):\n",
    "    tr_loss, tr_acc, tr_f1 = train_one_epoch_beats(model, tr_loader, opt, criterion, device=DEVICE)\n",
    "    va_loss, va_acc, va_f1, va_auc, va_aupr, best_thr = evaluate_by_record(model, va_loader, criterion, device=DEVICE)\n",
    "    print(f\"[Head] Ep{ep:02d} | tr_acc {tr_acc:.3f} f1 {tr_f1:.3f} | val_acc {va_acc:.3f} f1 {va_f1:.3f} auroc {va_auc:.3f} auprc {va_aupr:.3f}\")\n",
    "\n",
    "\n",
    "# Unfreeze last ResNet block + head with discriminative LRs\n",
    "for n, p in model.named_parameters():\n",
    "    if n.startswith(\"layer4\"):\n",
    "        p.requires_grad = True\n",
    "opt = torch.optim.AdamW([\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 1e-3},\n",
    "    {\"params\": [p for n,p in model.named_parameters() if n.startswith(\"layer4\") and p.requires_grad], \"lr\": 1e-4},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "for ep in range(1, 10+1):\n",
    "    tr_loss, tr_acc, tr_f1 = train_one_epoch_beats(model, tr_loader, opt, criterion, device=DEVICE)\n",
    "    va_loss, va_acc, va_f1, va_auc, va_aupr = evaluate_by_record(model, va_loader, criterion, device=DEVICE)\n",
    "    print(f\"[FT]   Ep{ep:02d} | tr_acc {tr_acc:.3f} f1 {tr_f1:.3f} | \"\n",
    "          f\"val_rec_acc {va_acc:.3f} f1 {va_f1:.3f} auroc {va_auc:.3f} auprc {va_aupr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3352e-ceb5-49dd-8d1f-2bf207f71f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
