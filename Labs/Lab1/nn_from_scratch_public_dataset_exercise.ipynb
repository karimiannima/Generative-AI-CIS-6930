{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Neural Network from Scratch \u2014 Public Dataset Exercise\n\nThis notebook adapts concepts from **Denny Britz\u2019s** tutorial \u201cImplementing a Neural Network from Scratch \u2014 An Introduction\u201d \nand turns them into a hands\u2011on exercise using public datasets.\n\n**Learning goals**\n- Implement a 3\u2011layer neural network (NumPy only \u2014 no PyTorch/TensorFlow).\n- Train on a **public dataset** (Digits or Iris) instead of random toy data.\n- Practice **forward pass**, **loss**, and **backpropagation**.\n- Evaluate accuracy and experiment with hyperparameters.\n\n> Attribution: inspired by Denny Britz, *Implementing a Neural Network from Scratch in Python (2015)*.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Setup\n\nRun the cell below to import the libraries we'll use. We\u2019ll stick to NumPy and a couple of utility libraries.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Standard\nimport math, time, random\nimport numpy as np\n\n# Data & utilities\nfrom sklearn.datasets import load_digits, load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Plotting (optional)\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1337)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Choose your public dataset\n\nWe\u2019ll avoid synthetic random blobs and use a small, public dataset that ships with scikit\u2011learn so no internet is required:\n\n- **Digits (8\u00d78 images, 10 classes)** \u2014 recommended (slightly larger, ~1797 samples).\n- **Iris (4 features, 3 classes)** \u2014 very small/easy.\n\n> \ud83d\udccc You can switch by changing `DATASET` to `\"digits\"` or `\"iris\"` below.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# TODO 1: Pick your dataset\nDATASET = \"digits\"   # options: \"digits\" or \"iris\"\n\nif DATASET == \"digits\":\n    data = load_digits()\n    X = data.data.astype(np.float64)        # shape: (n_samples, 64)\n    y = data.target.astype(int)             # labels: 0..9\nelif DATASET == \"iris\":\n    data = load_iris()\n    X = data.data.astype(np.float64)        # shape: (n_samples, 4)\n    y = data.target.astype(int)             # labels: 0..2\nelse:\n    raise ValueError(\"Unknown DATASET\")\n\nnum_classes = len(np.unique(y))\nprint(f\"Dataset: {DATASET}, X shape={X.shape}, classes={num_classes}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Train/Val split and (optional) PCA for visualization\n\nWe\u2019ll standardize features for faster training. For Digits (64\u2011D), we\u2019ll also compute a 2\u2011D PCA projection for plotting.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Split data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1337)\n\n# Standardize\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val   = scaler.transform(X_val)\n\n# Optional: 2D PCA projection for visualization\npca2 = PCA(n_components=2, random_state=1337)\nX_train_2d = pca2.fit_transform(X_train)\nX_val_2d   = pca2.transform(X_val)\n\nprint(\"Train:\", X_train.shape, \"Val:\", X_val.shape)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) From\u2011scratch 3\u2011layer Neural Network (NumPy)\n\nWe\u2019ll implement a simple fully\u2011connected network:\n- Input \u2192 Hidden (ReLU) \u2192 Output (softmax)\n- Loss: cross\u2011entropy\n- Parameters initialized with small random values.\n\n**Your tasks:**\n- Implement the **forward** pass (`forward`).\n- Compute the **loss** and **accuracy**.\n- Implement **backpropagation** (`backward`) to get gradients for `W1, b1, W2, b2`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# TODO 2: Complete the forward and backward passes\ndef one_hot(y, num_classes):\n    Y = np.zeros((y.size, num_classes), dtype=np.float64)\n    Y[np.arange(y.size), y] = 1.0\n    return Y\n\nclass MLP:\n    def __init__(self, input_dim, hidden_dim, num_classes, weight_scale=0.01):\n        # Parameters\n        self.W1 = np.random.randn(input_dim, hidden_dim) * weight_scale\n        self.b1 = np.zeros((1, hidden_dim), dtype=np.float64)\n        self.W2 = np.random.randn(hidden_dim, num_classes) * weight_scale\n        self.b2 = np.zeros((1, num_classes), dtype=np.float64)\n\n        # caches for backward\n        self.cache = {}\n\n    @staticmethod\n    def relu(z): \n        return np.maximum(0, z)\n\n    @staticmethod\n    def relu_grad(z):\n        return (z > 0).astype(np.float64)\n\n    def forward(self, X):\n        \"\"\"\n        TODO 2a:\n        - Compute z1 = X @ W1 + b1\n        - a1 = ReLU(z1)\n        - scores = a1 @ W2 + b2\n        - softmax probabilities p = exp(scores)/sum(exp(scores), axis=1, keepdims=True)\n        Return p, and cache intermediates needed for backward.\n        \"\"\"\n        # ----- Your code here -----\n        z1 = X @ self.W1 + self.b1\n        a1 = self.relu(z1)\n        scores = a1 @ self.W2 + self.b2\n        # numerical stability\n        scores -= scores.max(axis=1, keepdims=True)\n        exp_scores = np.exp(scores)\n        p = exp_scores / (exp_scores.sum(axis=1, keepdims=True) + 1e-12)\n        # cache\n        self.cache = {\"X\": X, \"z1\": z1, \"a1\": a1, \"p\": p}\n        return p\n\n    def loss(self, p, y):\n        \"\"\"Cross-entropy loss.\"\"\"\n        N = y.size\n        # prevent log(0)\n        log_likelihood = -np.log(p[np.arange(N), y] + 1e-12)\n        return log_likelihood.mean()\n\n    def backward(self, y):\n        \"\"\"\n        TODO 2b: Backpropagate through softmax+CE, linear layers, and ReLU.\n        Use cached values in self.cache. Compute grads for W2, b2, W1, b1.\n        Return a dict with the gradients.\n        \"\"\"\n        X  = self.cache[\"X\"]\n        z1 = self.cache[\"z1\"]\n        a1 = self.cache[\"a1\"]\n        p  = self.cache[\"p\"]\n        N  = y.size\n\n        # gradient of loss wrt scores\n        dscores = p.copy()\n        dscores[np.arange(N), y] -= 1.0\n        dscores /= N\n\n        # grads for W2, b2\n        dW2 = a1.T @ dscores\n        db2 = dscores.sum(axis=0, keepdims=True)\n\n        # backprop into a1\n        da1 = dscores @ self.W2.T\n        dz1 = da1 * self.relu_grad(z1)\n\n        # grads for W1, b1\n        dW1 = X.T @ dz1\n        db1 = dz1.sum(axis=0, keepdims=True)\n\n        return {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n\ndef accuracy(p, y):\n    preds = p.argmax(axis=1)\n    return (preds == y).mean()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Training loop (SGD)\n\nWe\u2019ll train with plain stochastic gradient descent (mini\u2011batches) and an optional L2 regularizer.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Hyperparameters (feel free to tune)\nhidden_dim = 64 if DATASET == \"digits\" else 16\nlr = 0.1\nweight_decay = 1e-4   # L2\nepochs = 30\nbatch_size = 64\n\nN, D = X_train.shape\nmlp = MLP(input_dim=D, hidden_dim=hidden_dim, num_classes=num_classes)\n\nY_train_oh = one_hot(y_train, num_classes)\n\ndef iterate_minibatches(X, y, bs, shuffle=True):\n    idx = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(idx)\n    for i in range(0, X.shape[0], bs):\n        j = idx[i:i+bs]\n        yield X[j], y[j]\n\nhistory = {\"tr_loss\": [], \"tr_acc\": [], \"val_loss\": [], \"val_acc\": []}\n\nfor epoch in range(1, epochs+1):\n    # Train\n    tr_loss_sum, tr_acc_sum, n_seen = 0.0, 0.0, 0\n    for xb, yb in iterate_minibatches(X_train, y_train, batch_size, shuffle=True):\n        # forward\n        p = mlp.forward(xb)\n        loss = mlp.loss(p, yb)\n        # L2 regularization\n        l2 = 0.5 * weight_decay * (np.sum(mlp.W1*mlp.W1) + np.sum(mlp.W2*mlp.W2))\n        loss_total = loss + l2\n        # backward\n        grads = mlp.backward(yb)\n        # add L2 grads\n        grads[\"W1\"] += weight_decay * mlp.W1\n        grads[\"W2\"] += weight_decay * mlp.W2\n        # SGD update\n        mlp.W1 -= lr * grads[\"W1\"]\n        mlp.b1 -= lr * grads[\"b1\"]\n        mlp.W2 -= lr * grads[\"W2\"]\n        mlp.b2 -= lr * grads[\"b2\"]\n        # stats\n        tr_loss_sum += float(loss) * xb.shape[0]\n        tr_acc_sum  += float(accuracy(p, yb)) * xb.shape[0]\n        n_seen += xb.shape[0]\n\n    # Evaluate\n    p_val = mlp.forward(X_val)\n    val_loss = mlp.loss(p_val, y_val)\n    tr_loss = tr_loss_sum / n_seen\n    tr_acc  = tr_acc_sum / n_seen\n    val_acc = accuracy(p_val, y_val)\n\n    history[\"tr_loss\"].append(tr_loss)\n    history[\"tr_acc\"].append(tr_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n\n    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {val_loss:.4f} acc {val_acc:.3f}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Plot training curves (optional)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Plot loss and accuracy over epochs\nfig, ax = plt.subplots(1, 2, figsize=(10,4))\nax[0].plot(history['tr_loss'], label='train')\nax[0].plot(history['val_loss'], label='val')\nax[0].set_title('Loss'); ax[0].set_xlabel('epoch'); ax[0].legend()\n\nax[1].plot(history['tr_acc'], label='train')\nax[1].plot(history['val_acc'], label='val')\nax[1].set_title('Accuracy'); ax[1].set_xlabel('epoch'); ax[1].legend()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) (Optional) 2D decision viz with PCA projection\n\nFor intuition only: we\u2019ll plot the 2D PCA projection of the validation set colored by predicted class.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Only meaningful for a quick qualitative look\np_val = mlp.forward(X_val)\npred_val = p_val.argmax(axis=1)\n\nplt.figure(figsize=(5,4))\nplt.scatter(X_val_2d[:,0], X_val_2d[:,1], c=pred_val, s=20)\nplt.title(f'Validation predictions (PCA\u20112D) \u2014 {DATASET}')\nplt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Experiments (student prompts)\n\n- **H1**: Increase `hidden_dim` and observe accuracy changes.\n- **H2**: Try a smaller learning rate (`lr=0.05`), then larger (`lr=0.2`).\n- **H3**: Remove weight decay (`weight_decay=0`) and compare overfitting.\n- **H4**: Switch from `digits` to `iris` and adapt `epochs`/`hidden_dim`.\n- **H5** *(challenge)*: Add another hidden layer.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## (Optional) Reference: Key equations\n\n- **Forward**\n  - $z_1 = XW_1 + b_1$\n  - $a_1 = \\max(0, z_1)$\n  - $s = a_1 W_2 + b_2$\n  - $p = \\text{softmax}(s)$\n\n- **Loss**\n  - $\\mathcal{L} = -\\frac{1}{N}\\sum_i \\log p_{i,y_i}$\n\n- **Backward (softmax+CE)**\n  - $\\frac{\\partial \\mathcal{L}}{\\partial s} = (p - \\text{one\\_hot}(y))/N$\n\nThen chain through linear and ReLU for $W_2, b_2, W_1, b_1$.\n"
    }
  ]
}