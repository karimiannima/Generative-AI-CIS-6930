{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae17d27c",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch — Public Dataset Exercise\n",
    "\n",
    "This notebook adapts concepts from **Denny Britz’s** tutorial “Implementing a Neural Network from Scratch — An Introduction” \n",
    "and turns them into a hands‑on exercise using public datasets.\n",
    "\n",
    "**Learning goals**\n",
    "- Implement a 3‑layer neural network (NumPy only — no PyTorch/TensorFlow).\n",
    "- Train on a **public dataset** (Digits or Iris) instead of random toy data.\n",
    "- Practice **forward pass**, **loss**, and **backpropagation**.\n",
    "- Evaluate accuracy and experiment with hyperparameters.\n",
    "\n",
    "> Attribution: inspired by Denny Britz, *Implementing a Neural Network from Scratch in Python (2015)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bf083",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "Run the cell below to import the libraries we'll use. We’ll stick to NumPy and a couple of utility libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import math, time, random\n",
    "import numpy as np\n",
    "\n",
    "# Data & utilities\n",
    "from sklearn.datasets import load_digits, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotting (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d7281",
   "metadata": {},
   "source": [
    "## 1) Choose your public dataset\n",
    "\n",
    "We’ll avoid synthetic random blobs and use a small, public dataset that ships with scikit‑learn so no internet is required:\n",
    "\n",
    "- **Digits (8×8 images, 10 classes)** — recommended (slightly larger, ~1797 samples).\n",
    "- **Iris (4 features, 3 classes)** — very small/easy.\n",
    "\n",
    "> 📌 You can switch by changing `DATASET` to `\"digits\"` or `\"iris\"` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Pick your dataset\n",
    "DATASET = \"digits\"   # options: \"digits\" or \"iris\"\n",
    "\n",
    "if DATASET == \"digits\":\n",
    "    data = load_digits()\n",
    "    X = data.data.astype(np.float64)        # shape: (n_samples, 64)\n",
    "    y = data.target.astype(int)             # labels: 0..9\n",
    "elif DATASET == \"iris\":\n",
    "    data = load_iris()\n",
    "    X = data.data.astype(np.float64)        # shape: (n_samples, 4)\n",
    "    y = data.target.astype(int)             # labels: 0..2\n",
    "else:\n",
    "    raise ValueError(\"Unknown DATASET\")\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"Dataset: {DATASET}, X shape={X.shape}, classes={num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238dd542",
   "metadata": {},
   "source": [
    "## 2) Train/Val split and (optional) PCA for visualization\n",
    "\n",
    "We’ll standardize features for faster training. For Digits (64‑D), we’ll also compute a 2‑D PCA projection for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e448ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1337)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "\n",
    "# Optional: 2D PCA projection for visualization\n",
    "pca2 = PCA(n_components=2, random_state=1337)\n",
    "X_train_2d = pca2.fit_transform(X_train)\n",
    "X_val_2d   = pca2.transform(X_val)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc312411",
   "metadata": {},
   "source": [
    "## 3) From‑scratch 3‑layer Neural Network (NumPy)\n",
    "\n",
    "We’ll implement a simple fully‑connected network:\n",
    "- Input → Hidden (ReLU) → Output (softmax)\n",
    "- Loss: cross‑entropy\n",
    "- Parameters initialized with small random values.\n",
    "\n",
    "**Your tasks:**\n",
    "- Implement the **forward** pass (`forward`).\n",
    "- Compute the **loss** and **accuracy**.\n",
    "- Implement **backpropagation** (`backward`) to get gradients for `W1, b1, W2, b2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Complete the forward and backward passes\n",
    "def one_hot(y, num_classes):\n",
    "    Y = np.zeros((y.size, num_classes), dtype=np.float64)\n",
    "    Y[np.arange(y.size), y] = 1.0\n",
    "    return Y\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, weight_scale=0.01):\n",
    "        # Parameters\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * weight_scale\n",
    "        self.b1 = np.zeros((1, hidden_dim), dtype=np.float64)\n",
    "        self.W2 = np.random.randn(hidden_dim, num_classes) * weight_scale\n",
    "        self.b2 = np.zeros((1, num_classes), dtype=np.float64)\n",
    "\n",
    "        # caches for backward\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z): \n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_grad(z):\n",
    "        return (z > 0).astype(np.float64)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        TODO 2a:\n",
    "        - Compute z1 = X @ W1 + b1\n",
    "        - a1 = ReLU(z1)\n",
    "        - scores = a1 @ W2 + b2\n",
    "        - softmax probabilities p = exp(scores)/sum(exp(scores), axis=1, keepdims=True)\n",
    "        Return p, and cache intermediates needed for backward.\n",
    "        \"\"\"\n",
    "        # ----- Your code here -----\n",
    "        z1 = \n",
    "        a1 = \n",
    "        scores = \n",
    "        # numerical stability\n",
    "        scores -= \n",
    "        exp_scores = \n",
    "        p = \n",
    "        # cache\n",
    "        self.cache = \n",
    "        return p\n",
    "\n",
    "    def loss(self, p, y):\n",
    "        \"\"\"Cross-entropy loss.\"\"\"\n",
    "        N = y.size\n",
    "        # prevent log(0)\n",
    "        log_likelihood = -np.log(p[np.arange(N), y] + 1e-12)\n",
    "        return log_likelihood.mean()\n",
    "\n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        TODO 2b: Backpropagate through softmax+CE, linear layers, and ReLU.\n",
    "        Use cached values in self.cache. Compute grads for W2, b2, W1, b1.\n",
    "        Return a dict with the gradients.\n",
    "        \"\"\"\n",
    "        # ----- Your code here -----\n",
    "\n",
    "        X  = self.cache[]\n",
    "        z1 = self.cache[]\n",
    "        a1 = self.cache[]\n",
    "        p  = self.cache[]\n",
    "        N  = y.size\n",
    "\n",
    "        # gradient of loss wrt scores\n",
    "        dscores = p.copy()\n",
    "        dscores[np.arange(N), y] -= 1.0\n",
    "        dscores /= N\n",
    "\n",
    "        # grads for W2, b2\n",
    "        dW2 = a1.T @ dscores\n",
    "        db2 = dscores.sum(axis=0, keepdims=True)\n",
    "\n",
    "        # backprop into a1\n",
    "        da1 = dscores @ self.W2.T\n",
    "        dz1 = da1 * self.relu_grad(z1)\n",
    "\n",
    "        # grads for W1, b1\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = dz1.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "\n",
    "def accuracy(p, y):\n",
    "    preds = p.argmax(axis=1)\n",
    "    return (preds == y).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e9c18",
   "metadata": {},
   "source": [
    "## 4) Training loop (SGD)\n",
    "\n",
    "We’ll train with plain stochastic gradient descent (mini‑batches) and an optional L2 regularizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a40970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (feel free to tune)\n",
    "hidden_dim = 64 if DATASET == \"digits\" else 16\n",
    "lr = 0.1\n",
    "weight_decay = 1e-4   # L2\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "N, D = X_train.shape\n",
    "mlp = MLP(input_dim=D, hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "\n",
    "Y_train_oh = one_hot(y_train, num_classes)\n",
    "\n",
    "def iterate_minibatches(X, y, bs, shuffle=True):\n",
    "    idx = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, X.shape[0], bs):\n",
    "        j = idx[i:i+bs]\n",
    "        yield X[j], y[j]\n",
    "\n",
    "history = {\"tr_loss\": [], \"tr_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # Train\n",
    "    tr_loss_sum, tr_acc_sum, n_seen = 0.0, 0.0, 0\n",
    "    for xb, yb in iterate_minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        # forward\n",
    "        p = mlp.forward(xb)\n",
    "        loss = mlp.loss(p, yb)\n",
    "        # L2 regularization\n",
    "        l2 = 0.5 * weight_decay * (np.sum(mlp.W1*mlp.W1) + np.sum(mlp.W2*mlp.W2))\n",
    "        loss_total = loss + l2\n",
    "        # backward\n",
    "        grads = mlp.backward(yb)\n",
    "        # add L2 grads\n",
    "        grads[\"W1\"] += weight_decay * mlp.W1\n",
    "        grads[\"W2\"] += weight_decay * mlp.W2\n",
    "         # ----- Your code here -----\n",
    "\n",
    "        # SGD update\n",
    "        mlp.W1 -= lr * grads[\"  \"]\n",
    "        mlp.b1 -= lr * grads[\"  \"]\n",
    "        mlp.W2 -= lr * grads[\"  \"]\n",
    "        mlp.b2 -= lr * grads[\"  \"]\n",
    "        # stats\n",
    "        tr_loss_sum += float(loss) * xb.shape[0]\n",
    "        tr_acc_sum  += float(accuracy(p, yb)) * xb.shape[0]\n",
    "        n_seen += xb.shape[0]\n",
    "\n",
    "    # Evaluate\n",
    "    p_val = mlp.forward(X_val)\n",
    "    val_loss = mlp.loss(p_val, y_val)\n",
    "    tr_loss = tr_loss_sum / n_seen\n",
    "    tr_acc  = tr_acc_sum / n_seen\n",
    "    val_acc = accuracy(p_val, y_val)\n",
    "\n",
    "    history[\"tr_loss\"].append(tr_loss)\n",
    "    history[\"tr_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {val_loss:.4f} acc {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9336c",
   "metadata": {},
   "source": [
    "## 5) Plot training curves (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy over epochs\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "ax[0].plot(history['tr_loss'], label='train')\n",
    "ax[0].plot(history['val_loss'], label='val')\n",
    "ax[0].set_title('Loss'); ax[0].set_xlabel('epoch'); ax[0].legend()\n",
    "\n",
    "ax[1].plot(history['tr_acc'], label='train')\n",
    "ax[1].plot(history['val_acc'], label='val')\n",
    "ax[1].set_title('Accuracy'); ax[1].set_xlabel('epoch'); ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fbf054",
   "metadata": {},
   "source": [
    "## 6) (Optional) 2D decision viz with PCA projection\n",
    "\n",
    "For intuition only: we’ll plot the 2D PCA projection of the validation set colored by predicted class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6827fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only meaningful for a quick qualitative look\n",
    "p_val = mlp.forward(X_val)\n",
    "pred_val = p_val.argmax(axis=1)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_val_2d[:,0], X_val_2d[:,1], c=pred_val, s=20)\n",
    "plt.title(f'Validation predictions (PCA‑2D) — {DATASET}')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f61d7",
   "metadata": {},
   "source": [
    "## 7) Experiments (student prompts)\n",
    "\n",
    "- **H1**: Increase `hidden_dim` and observe accuracy changes.\n",
    "- **H2**: Try a smaller learning rate (`lr=0.05`), then larger (`lr=0.2`).\n",
    "- **H3**: Remove weight decay (`weight_decay=0`) and compare overfitting.\n",
    "- **H4**: Switch from `digits` to `iris` and adapt `epochs`/`hidden_dim`.\n",
    "- **H5** *(challenge)*: Add another hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd9e54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## (Optional) Reference: Key equations\n",
    "\n",
    "- **Forward**\n",
    "  - $z_1 = XW_1 + b_1$\n",
    "  - $a_1 = \\max(0, z_1)$\n",
    "  - $s = a_1 W_2 + b_2$\n",
    "  - $p = \\text{softmax}(s)$\n",
    "\n",
    "- **Loss**\n",
    "  - $\\mathcal{L} = -\\frac{1}{N}\\sum_i \\log p_{i,y_i}$\n",
    "\n",
    "- **Backward (softmax+CE)**\n",
    "  - $\\frac{\\partial \\mathcal{L}}{\\partial s} = (p - \\text{one\\_hot}(y))/N$\n",
    "\n",
    "Then chain through linear and ReLU for $W_2, b_2, W_1, b_1$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
