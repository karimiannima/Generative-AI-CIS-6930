{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df1d6559-9bc7-4e91-83c1-79628cb319d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: Boston (OpenML) | X shape: (506, 13), y shape: (506,)\n",
      "\n",
      "[Ridge] best params: {'model__alpha': np.float64(17.78279410038923)}\n",
      "[Lasso] best params: {'model__alpha': np.float64(0.03162277660168379)}\n",
      "[ElasticNet] best params: {'model__alpha': np.float64(0.0517947467923121), 'model__l1_ratio': 0.3}\n",
      "\n",
      "=== Comparison — Boston (OpenML) ===\n",
      "\n",
      "                               model  train_acc(%)  test_acc(%)  test_RMSE  test_MAE\n",
      "Polynomial (deg=2) + ElasticNet (CV)         93.08        80.42     4.0380    2.6187\n",
      "     Polynomial (deg=2) + Ridge (CV)         93.24        79.99     4.0815    2.6498\n",
      "     Polynomial (deg=2) + Lasso (CV)         93.42        79.26     4.1555    2.6486\n",
      "                              Linear         76.45        67.34     5.2150    3.6099\n",
      "         Polynomial (deg=2) + Linear         95.17        64.87     5.4086    3.0520\n"
     ]
    }
   ],
   "source": [
    "# compare_linear_regularization_boston_or_california_with_accuracy.py\n",
    "# Linear vs Polynomial Linear vs Ridge (L2) vs Lasso (L1) vs Elastic-Net (L1+L2)\n",
    "# Reports TRAIN/TEST \"Accuracy\" as R²×100, plus Test RMSE/MAE.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# -----------------------------\n",
    "# Data loader (Boston -> fallback to California)\n",
    "# -----------------------------\n",
    "def load_boston_or_california():\n",
    "    try:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "        boston = fetch_openml(name=\"boston\", version=1, as_frame=False)\n",
    "        X = boston.data\n",
    "        y = boston.target.astype(np.float64)\n",
    "        feature_names = boston.feature_names\n",
    "        return X, y, feature_names, \"Boston (OpenML)\"\n",
    "    except Exception:\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        cal = fetch_california_housing(as_frame=False)\n",
    "        X = cal.data\n",
    "        y = cal.target.astype(np.float64)\n",
    "        feature_names = cal.feature_names\n",
    "        return X, y, feature_names, \"California Housing (fallback)\"\n",
    "\n",
    "X, y, feature_names, dataset_name = load_boston_or_california()\n",
    "print(f\"Loaded dataset: {dataset_name} | X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=0\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def test_metrics(y_true, y_pred):\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    return rmse, mae\n",
    "\n",
    "def add_result(name, model, fit=True, store=[]):\n",
    "    if fit:\n",
    "        model.fit(X_train, y_train)\n",
    "    # \"Accuracy\" for regression = R² × 100\n",
    "    train_acc = model.score(X_train, y_train) * 100.0\n",
    "    test_acc  = model.score(X_test,  y_test)  * 100.0\n",
    "    y_te = model.predict(X_test)\n",
    "    te_rmse, te_mae = test_metrics(y_test, y_te)\n",
    "    store.append({\n",
    "        \"model\": name,\n",
    "        \"train_acc(%)\": round(train_acc, 2),\n",
    "        \"test_acc(%)\":  round(test_acc,  2),\n",
    "        \"test_RMSE\":    round(te_rmse,   4),\n",
    "        \"test_MAE\":     round(te_mae,    4),\n",
    "    })\n",
    "    return store\n",
    "\n",
    "rows = []\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Basic Linear Regression (no scaling, no poly)\n",
    "# -----------------------------\n",
    "lin_basic = LinearRegression()\n",
    "rows = add_result(\"Linear\", lin_basic, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Polynomial Linear Regression (deg=2)\n",
    "# -----------------------------\n",
    "poly_lin = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "rows = add_result(\"Polynomial (deg=2) + Linear\", poly_lin, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Ridge (L2) + Polynomial(deg=2), CV on alpha\n",
    "# -----------------------------\n",
    "ridge_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", Ridge())\n",
    "])\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge_pipe,\n",
    "    param_grid={\"model__alpha\": np.logspace(-3, 3, 25)},\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5, n_jobs=-1, refit=True\n",
    ")\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "print(\"\\n[Ridge] best params:\", ridge_grid.best_params_)\n",
    "rows = add_result(\"Polynomial (deg=2) + Ridge (CV)\", ridge_grid.best_estimator_, fit=False, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Lasso (L1) + Polynomial(deg=2), CV on alpha\n",
    "# -----------------------------\n",
    "lasso_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", Lasso(max_iter=10000))\n",
    "])\n",
    "lasso_grid = GridSearchCV(\n",
    "    lasso_pipe,\n",
    "    param_grid={\"model__alpha\": np.logspace(-3, 1, 25)},\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5, n_jobs=-1, refit=True\n",
    ")\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "print(\"[Lasso] best params:\", lasso_grid.best_params_)\n",
    "rows = add_result(\"Polynomial (deg=2) + Lasso (CV)\", lasso_grid.best_estimator_, fit=False, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Elastic-Net + Polynomial(deg=2), CV on alpha & l1_ratio\n",
    "# -----------------------------\n",
    "enet_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", ElasticNet(max_iter=10000))\n",
    "])\n",
    "enet_grid = GridSearchCV(\n",
    "    enet_pipe,\n",
    "    param_grid={\n",
    "        \"model__alpha\": np.logspace(-3, 1, 15),\n",
    "        \"model__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    },\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5, n_jobs=-1, refit=True\n",
    ")\n",
    "enet_grid.fit(X_train, y_train)\n",
    "print(\"[ElasticNet] best params:\", enet_grid.best_params_)\n",
    "rows = add_result(\"Polynomial (deg=2) + ElasticNet (CV)\", enet_grid.best_estimator_, fit=False, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# Results table (sorted by Test Accuracy)\n",
    "# -----------------------------\n",
    "res = pd.DataFrame(rows).sort_values(\"test_acc(%)\", ascending=False)\n",
    "print(\"\\n=== Comparison —\", dataset_name, \"===\\n\")\n",
    "print(res.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c22d38ac-6ce5-43e6-9b9e-38a31f7aaed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: California Housing | X shape: (20640, 8), y shape: (20640,)\n",
      "\n",
      "[Ridge] best params: {'model__alpha': np.float64(177.82794100389228)}\n",
      "[Lasso] best params: {'model__alpha': np.float64(0.01467799267622069)}\n",
      "[ElasticNet] best params: {'model__alpha': np.float64(0.026826957952797246), 'model__l1_ratio': 0.1}\n",
      "\n",
      "=== Comparison — California Housing ===\n",
      "\n",
      "                               model  train_acc(%)  test_acc(%)  test_RMSE  test_MAE\n",
      "     Polynomial (deg=2) + Ridge (CV)         67.07        64.62     0.6868    0.4823\n",
      "                              Linear         61.13        59.26     0.7370    0.5362\n",
      "Polynomial (deg=2) + ElasticNet (CV)         64.58        45.69     0.8510    0.5139\n",
      "     Polynomial (deg=2) + Lasso (CV)         62.90        -8.28     1.2015    0.5370\n",
      "         Polynomial (deg=2) + Linear         68.62       -60.29     1.4618    0.4791\n"
     ]
    }
   ],
   "source": [
    "# compare_linear_regularization_california.py\n",
    "# Linear vs Polynomial Linear vs Ridge (L2) vs Lasso (L1) vs Elastic-Net (L1+L2)\n",
    "# Dataset: California Housing\n",
    "# Reports TRAIN/TEST \"Accuracy\" as R²×100, plus Test RMSE/MAE.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# -----------------------------\n",
    "# Data: California Housing\n",
    "# -----------------------------\n",
    "cal = fetch_california_housing(as_frame=False)\n",
    "X = cal.data\n",
    "y = cal.target.astype(np.float64)\n",
    "feature_names = cal.feature_names\n",
    "dataset_name = \"California Housing\"\n",
    "print(f\"Loaded dataset: {dataset_name} | X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=0\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def test_metrics(y_true, y_pred):\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    return rmse, mae\n",
    "\n",
    "def add_result(name, model, fit=True, store=[]):\n",
    "    if fit:\n",
    "        model.fit(X_train, y_train)\n",
    "    # \"Accuracy\" for regression = R² × 100\n",
    "    train_acc = model.score(X_train, y_train) * 100.0\n",
    "    test_acc  = model.score(X_test,  y_test)  * 100.0\n",
    "    y_te = model.predict(X_test)\n",
    "    te_rmse, te_mae = test_metrics(y_test, y_te)\n",
    "    store.append({\n",
    "        \"model\": name,\n",
    "        \"train_acc(%)\": round(train_acc, 2),\n",
    "        \"test_acc(%)\":  round(test_acc,  2),\n",
    "        \"test_RMSE\":    round(te_rmse,   4),\n",
    "        \"test_MAE\":     round(te_mae,    4),\n",
    "    })\n",
    "    return store\n",
    "\n",
    "rows = []\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Basic Linear Regression (no scaling, no poly)\n",
    "# -----------------------------\n",
    "lin_basic = LinearRegression()\n",
    "rows = add_result(\"Linear\", lin_basic, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Polynomial Linear Regression (deg=2)\n",
    "# -----------------------------\n",
    "poly_lin = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "rows = add_result(\"Polynomial (deg=2) + Linear\", poly_lin, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Ridge (L2) + Polynomial(deg=2), CV on alpha\n",
    "# -----------------------------\n",
    "ridge_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", Ridge())\n",
    "])\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge_pipe,\n",
    "    param_grid={\"model__alpha\": np.logspace(-3, 3, 25)},\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5, n_jobs=-1, refit=True\n",
    ")\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "print(\"\\n[Ridge] best params:\", ridge_grid.best_params_)\n",
    "rows = add_result(\"Polynomial (deg=2) + Ridge (CV)\", ridge_grid.best_estimator_, fit=False, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Lasso (L1) + Polynomial(deg=2), CV on alpha\n",
    "# -----------------------------\n",
    "lasso_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", Lasso(max_iter=10000))\n",
    "])\n",
    "lasso_grid = GridSearchCV(\n",
    "    lasso_pipe,\n",
    "    param_grid={\"model__alpha\": np.logspace(-3, 1, 25)},\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5, n_jobs=-1, refit=True\n",
    ")\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "print(\"[Lasso] best params:\", lasso_grid.best_params_)\n",
    "rows = add_result(\"Polynomial (deg=2) + Lasso (CV)\", lasso_grid.best_estimator_, fit=False, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Elastic-Net + Polynomial(deg=2), CV on alpha & l1_ratio\n",
    "# -----------------------------\n",
    "enet_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", ElasticNet(max_iter=10000))\n",
    "])\n",
    "enet_grid = GridSearchCV(\n",
    "    enet_pipe,\n",
    "    param_grid={\n",
    "        \"model__alpha\": np.logspace(-3, 1, 15),\n",
    "        \"model__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    },\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5, n_jobs=-1, refit=True\n",
    ")\n",
    "enet_grid.fit(X_train, y_train)\n",
    "print(\"[ElasticNet] best params:\", enet_grid.best_params_)\n",
    "rows = add_result(\"Polynomial (deg=2) + ElasticNet (CV)\", enet_grid.best_estimator_, fit=False, store=rows)\n",
    "\n",
    "# -----------------------------\n",
    "# Results table (sorted by Test Accuracy)\n",
    "# -----------------------------\n",
    "res = pd.DataFrame(rows).sort_values(\"test_acc(%)\", ascending=False)\n",
    "print(\"\\n=== Comparison — California Housing ===\\n\")\n",
    "print(res.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f900152c-b54c-42cb-bc97-aeb20b15ce3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': np.float64(100.0), 'clf__penalty': 'l2'}\n",
      "Test accuracy: 0.6764\n",
      "Penalty: L2 | Non-zero coefficients: 2459531/2461460\n",
      "L2 → C=100.0 | Test acc=0.6763 | Non-zero coefs=2460936/2461460\n",
      "L1 → C=0.001 | Test acc=0.0524 | Non-zero coefs=0/2461460\n"
     ]
    }
   ],
   "source": [
    "# l1_vs_l2_20newsgroups.py     L1 vs L2 on 20 Newsgroups (Logistic Regression)\n",
    "# Compare L1 vs L2 regularization on high-dimensional sparse text\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1) Load data\n",
    "train = fetch_20newsgroups(subset=\"train\", remove=(\"headers\",\"footers\",\"quotes\"))\n",
    "test  = fetch_20newsgroups(subset=\"test\",  remove=(\"headers\",\"footers\",\"quotes\"))\n",
    "\n",
    "# 2) Pipeline: TF-IDF -> LogisticRegression(saga)\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9)),\n",
    "    (\"clf\",   LogisticRegression(solver=\"saga\", max_iter=5000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# 3) Hyperparameters: L2 vs L1 and strength C\n",
    "Cgrid = np.logspace(-3, 2, 8)\n",
    "param_grid = [\n",
    "    {\"clf__penalty\": [\"l2\"], \"clf__C\": Cgrid},\n",
    "    {\"clf__penalty\": [\"l1\"], \"clf__C\": Cgrid},\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe, param_grid, cv=5, n_jobs=-1, scoring=\"accuracy\", refit=True, verbose=0\n",
    ")\n",
    "gs.fit(train.data, train.target)\n",
    "\n",
    "best = gs.best_estimator_\n",
    "y_pred = best.predict(test.data)\n",
    "acc = accuracy_score(test.target, y_pred)\n",
    "\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# 4) Coefficient sparsity (meaningful for L1)\n",
    "clf = best.named_steps[\"clf\"]\n",
    "is_l1 = clf.penalty == \"l1\"\n",
    "try:\n",
    "    # number of features equals length of vectorizer vocab\n",
    "    n_feats = len(best.named_steps[\"tfidf\"].get_feature_names_out())\n",
    "except Exception:\n",
    "    n_feats = clf.coef_.shape[1]\n",
    "\n",
    "coefs = clf.coef_.ravel()\n",
    "n_zeros = int(np.isclose(coefs, 0.0).sum())\n",
    "print(f\"Penalty: {clf.penalty.upper()} | Non-zero coefficients: {coefs.size - n_zeros}/{coefs.size}\")\n",
    "\n",
    "# 5) Quick side-by-side: refit best C under the *other* penalty for comparison\n",
    "def score_for(penalty):\n",
    "    # choose the best C found for that penalty; if missing, reuse gs.best_params_['clf__C']\n",
    "    C_candidates = [p[\"clf__C\"] for p in gs.cv_results_[\"params\"] if p[\"clf__penalty\"] == penalty]\n",
    "    C_use = gs.best_params_[\"clf__C\"] if penalty == gs.best_params_[\"clf__penalty\"] else (C_candidates[0] if C_candidates else 1.0)\n",
    "    alt = Pipeline([\n",
    "        (\"tfidf\", best.named_steps[\"tfidf\"]),\n",
    "        (\"clf\",   LogisticRegression(solver=\"saga\", max_iter=5000, n_jobs=-1, penalty=penalty, C=C_use))\n",
    "    ]).fit(train.data, train.target)\n",
    "    pred = alt.predict(test.data)\n",
    "    acc  = accuracy_score(test.target, pred)\n",
    "    co   = alt.named_steps[\"clf\"].coef_.ravel()\n",
    "    zeros = int(np.isclose(co, 0.0).sum())\n",
    "    print(f\"{penalty.upper()} → C={C_use} | Test acc={acc:.4f} | Non-zero coefs={co.size - zeros}/{co.size}\")\n",
    "\n",
    "score_for(\"l2\")\n",
    "score_for(\"l1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f2ba0-1fa9-4857-a186-d0ed6fe7e1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
